{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e2debc-1374-4624-ade1-60d9a051995e",
   "metadata": {},
   "source": [
    "# Compare Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39428d",
   "metadata": {},
   "source": [
    "We compare the streams generated with \n",
    "\n",
    "1. controlled lexicons (ours),\n",
    "2. random baseline streams, and\n",
    "3. streams generated based on reference lexicons from the literature\n",
    "\n",
    "based on the repetitiveness of the phoneme features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7f68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_SSML = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e7ba-dc5a-4e87-923a-c9423d498d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate German Syllables: ɡaː|ɡiː|ɡyː|ɡɛː|kaː|koː|kuː|køː|kɛː|baː|... (75 elements total)\n",
      "Generate English Syllables: cʔː|cɥː|cɰː|cʋː|cʍː|cjː|cwː|cɹː|cɻː|cɑː|... (2294 elements total)\n",
      "Generate German Words: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 847.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: siːmoːɡaː|ʃaːhiːbyː|nuːɡɛːfyː|køːnoːvaː|siːmɛːkoː|hiːʃaːbøː|ɡaːluːfyː|ɡaːlyːfiː|doːfaːhiː|lyːçaːbøː|... (10000 elements total)\n",
      "Generate English Words: ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:52<00:00, 191.40it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "To check for phoneme position probability, your phonemes need the 'word_position_prob' info key. Before creating syllables and words from your phonemes run `phonemes = phonemes.intersection(read_phoneme_corpus())`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m german_words\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgerman_words.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate English Words: ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m english_words \u001b[38;5;241m=\u001b[39m \u001b[43mmake_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_syllables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menglish_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m english_words\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_words.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/core/word.py:140\u001b[0m, in \u001b[0;36mmake_words\u001b[0;34m(syllables, num_syllables, bigram_control, bigram_alpha, trigram_control, trigram_alpha, positional_control, positional_control_position, position_alpha, phonotactic_control, n_look_back, n_words, max_tries, progress_bar, lang)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positional_control \u001b[38;5;129;01mand\u001b[39;00m lang\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    139\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional control...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     words_register \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_common_phoneme_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositional_control_position\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words_register\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/controls/filter.py:66\u001b[0m, in \u001b[0;36mfilter_common_phoneme_words\u001b[0;34m(words, position, p_threshold, ipa_seg_path)\u001b[0m\n\u001b[1;32m     63\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExclude words with low (onset) syllable probability.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_common_phonemes_at_all_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m words\u001b[38;5;241m.\u001b[39mfilter(filter_common_phonemes_at_position, position\u001b[38;5;241m=\u001b[39mposition, p_threshold\u001b[38;5;241m=\u001b[39mp_threshold)\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/types/base_types.py:142\u001b[0m, in \u001b[0;36mRegister.filter\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty_like()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    143\u001b[0m         reg\u001b[38;5;241m.\u001b[39mappend(element)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reg\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/controls/filter.py:54\u001b[0m, in \u001b[0;36mfilter_common_phonemes_at_all_positions\u001b[0;34m(word, p_threshold)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_common_phonemes_at_all_positions\u001b[39m(word: Word, p_threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     53\u001b[0m     phonemes \u001b[38;5;241m=\u001b[39m [phoneme \u001b[38;5;28;01mfor\u001b[39;00m syllable \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m phoneme \u001b[38;5;129;01min\u001b[39;00m syllable]\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphoneme_is_common_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43mph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphonemes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/controls/filter.py:54\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfilter_common_phonemes_at_all_positions\u001b[39m(word: Word, p_threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     53\u001b[0m     phonemes \u001b[38;5;241m=\u001b[39m [phoneme \u001b[38;5;28;01mfor\u001b[39;00m syllable \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m phoneme \u001b[38;5;129;01min\u001b[39;00m syllable]\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mphoneme_is_common_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43mph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_threshold\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m position, ph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(phonemes))\n",
      "File \u001b[0;32m~/workspace/arpac/src/arpac/controls/filter.py:44\u001b[0m, in \u001b[0;36mphoneme_is_common_at\u001b[0;34m(phoneme, position, p_threshold)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mphoneme_is_common_at\u001b[39m(phoneme: Phoneme, position: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, p_threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_position_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m phoneme\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mkeys(), (\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo check for phoneme position probability, your phonemes need the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_position_prob\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m info key. \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore creating syllables and words from your phonemes run `phonemes = phonemes.intersection(read_phoneme_corpus())`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m phoneme\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_position_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(position, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p_threshold\n",
      "\u001b[0;31mAssertionError\u001b[0m: To check for phoneme position probability, your phonemes need the 'word_position_prob' info key. Before creating syllables and words from your phonemes run `phonemes = phonemes.intersection(read_phoneme_corpus())`."
     ]
    }
   ],
   "source": [
    "from arpac import load_phonemes, make_syllables, make_words\n",
    "\n",
    "import os\n",
    "\n",
    "results_path = \"results\"\n",
    "\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "german_syllables = make_syllables(load_phonemes(language_control=True), lang=\"deu\")\n",
    "print(f\"Generate German Syllables: {german_syllables}\")\n",
    "german_syllables.save(os.path.join(results_path, \"german_syllables.json\"))\n",
    "\n",
    "english_syllables = make_syllables(load_phonemes(language_control=False), lang=\"eng\")\n",
    "print(f\"Generate English Syllables: {english_syllables}\")\n",
    "english_syllables.save(os.path.join(results_path, \"english_syllables.json\"))\n",
    "\n",
    "if EXPORT_SSML:\n",
    "    from arpac.io import export_speech_synthesizer\n",
    "    export_speech_synthesizer(german_syllables, syllables_dir=os.path.join(results_path, \"ssml\", \"german\"))\n",
    "    export_speech_synthesizer(english_syllables, syllables_dir=os.path.join(results_path, \"ssml\", \"english\"))\n",
    "\n",
    "print(f\"Generate German Words: ...\")\n",
    "german_words = make_words(german_syllables)\n",
    "print(f\"Words: {german_words}\")\n",
    "german_words.save(os.path.join(results_path, \"german_words.json\"))\n",
    "\n",
    "print(f\"Generate English Words: ...\")\n",
    "english_words = make_words(english_syllables, lang=\"eng\")\n",
    "print(f\"Words: {english_words}\")\n",
    "english_words.save(os.path.join(results_path, \"english_words.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1cfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import pandas as pd\n",
    "from arpac.types.base_types import Register\n",
    "import numpy as np\n",
    "\n",
    "from arpac.core.syllable import LABELS_C, LABELS_V, syllable_from_phonemes\n",
    "from arpac.core.word import Word, word_overlap_matrix\n",
    "\n",
    "phonemes = load_phonemes()\n",
    "\n",
    "syll_feature_labels = [LABELS_C, LABELS_V]\n",
    "syllable_type = \"cv\"\n",
    "\n",
    "def to_phoneme(phoneme):\n",
    "    return phoneme\n",
    "\n",
    "def to_syllable(syllable):\n",
    "    if len(syllable) == 3 and not syllable.endswith(\"ː\"):\n",
    "        syllable_obj = syllable_from_phonemes(phonemes, syllable[:2], syll_feature_labels)\n",
    "        syllable_obj.id = syllable\n",
    "        return syllable_obj\n",
    "    return syllable_from_phonemes(phonemes, syllable, syll_feature_labels)\n",
    "\n",
    "def to_word(word):\n",
    "    syllables_list = list(map(to_syllable, word))\n",
    "    word_id = \"\".join(s.id for s in syllables_list)\n",
    "    word_features = list(list(tup) for tup in zip(*[s.info[\"binary_features\"] for s in syllables_list]))\n",
    "    return Word(id=word_id, info={\"binary_features\": word_features}, syllables=syllables_list)\n",
    "\n",
    "def to_lexicon(lexicon):\n",
    "    word_objs_list = list(map(to_word, lexicon))\n",
    "    lexicon = Register({w.id:  w for w in word_objs_list})\n",
    "    lexicon.info.update({\"syllable_feature_labels\": [LABELS_C, LABELS_V],  \"syllable_type\": syllable_type})\n",
    "    overlap = word_overlap_matrix(lexicon)\n",
    "    lexicon.info[\"cumulative_feature_repetitiveness\"] = np.triu(overlap, 1).sum()\n",
    "    lexicon.info[\"max_pairwise_feature_repetitiveness\"] = np.triu(overlap, 1).max()\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f78b01-78c8-4ba3-8844-c292cbe5b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LEXICONS = 21  # number of lexicons per TP mode\n",
    "N_WORDS_PER_LEXICON = 4  \n",
    "N_REPS = 5  # how often to randomize the lexicon to build the total stream, \n",
    "            # i.e. how long will the streams be in lexicon lengths N_REPS*N_WORDS_PER_LEXICON = n words in the stream\n",
    "N_STREAMS_PER_INPUT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742895df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream_info(stream):\n",
    "    print(\"Stream:\", \"|\".join([syll.id for syll in stream]))\n",
    "    print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "    print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "    print(\"Feature PRIs:\", stream.info[\"rhythmicity_indexes\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764ef8b",
   "metadata": {},
   "source": [
    "### arpac Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337f24-4b48-4598-ad47-00714b74921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arpac import make_streams\n",
    "from arpac import make_lexicons, Register, load_phonemes, load_words\n",
    "\n",
    "print(\"Load words...\")\n",
    "words = load_words(\"words.json\")\n",
    "\n",
    "print(\"Make lexicons...\")\n",
    "controlled_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=True)\n",
    "\n",
    "for i, lex in enumerate(controlled_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", controlled_lexicons[0])\n",
    "print(\"Info:\", controlled_lexicons[0].info)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87529389",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        controlled_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        controlled_streams.append(stream)\n",
    "\n",
    "print_stream_info(controlled_streams[0])\n",
    "\n",
    "len(controlled_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00475fed-bf37-4821-87a7-6962ef72145d",
   "metadata": {},
   "source": [
    "### Random / uncontrolled lexicons (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_lexicons = make_lexicons(words, n_lexicons=N_LEXICONS, n_words=N_WORDS_PER_LEXICON, control_features=False)\n",
    "\n",
    "for i, lex in enumerate(random_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", random_lexicons[0])\n",
    "print(\"Info:\", random_lexicons[0].info)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16a898-0eeb-4519-ad2e-ad625da4b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        random_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        random_streams.append(stream)\n",
    "        \n",
    "print_stream_info(random_streams[0])\n",
    "\n",
    "len(random_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc5056",
   "metadata": {},
   "source": [
    "### Reference lexicons from the literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490522",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicons = [\n",
    " [['pi', 'ɾu', 'ta'],\n",
    "  ['ba', 'ɡo', 'li'],\n",
    "  ['to', 'ku', 'da'],\n",
    "  ['ɡu', 'ki', 'bo']],\n",
    " [['pa', 'be', 'la'],\n",
    "  ['di', 'ne', 'ka'],\n",
    "  ['lu', 'fa', 'ri'],\n",
    "  ['xi', 'so', 'du']],\n",
    " [['ma', 'xu', 'pe'],\n",
    "  ['xe', 'ro', 'ɡa'],\n",
    "  ['de', 'mu', 'si'],\n",
    "  ['fo', 'le', 'ti']],\n",
    " [['pu', 'ke', 'mi'],\n",
    "  ['ra', 'fi', 'nu'],\n",
    "  ['bi', 'na', 'po'],\n",
    "  ['me', 'do', 'xi']],\n",
    " [['no', 'ni', 'xe'],\n",
    "  ['bu', 'lo', 'te'],\n",
    "  ['re', 'mo', 'fu'],\n",
    "  ['ko', 'tu', 'sa']],\n",
    " [['mi', 'lo', 'de'],\n",
    "  ['da', 'le', 'bu'],\n",
    "  ['no', 'ru', 'pa'],\n",
    "  ['ka', 'te', 'xi']],\n",
    " [['ne', 'do', 'li'],\n",
    "  ['ri', 'fo', 'nu'],\n",
    "  ['ba', 'to', 'ɡu'],\n",
    "  ['ki', 'ra', 'pu']],\n",
    " [['ɡo', 'na', 'be'],\n",
    "  ['mu', 'di', 'la'],\n",
    "  ['ro', 'ni', 'xe'],\n",
    "  ['pi', 'ku', 'sa']],\n",
    " [['fu', 'bi', 're'],\n",
    "  ['xe', 'tu', 'si'],\n",
    "  ['ta', 'fi', 'ko'],\n",
    "  ['ke', 'ma', 'po']],\n",
    " [['ti', 'fa', 'xu'],\n",
    "  ['so', 'du', 'xi'],\n",
    "  ['me', 'lu', 'bo'],\n",
    "  ['ɡa', 'ni', 'pe']],\n",
    " [['mi', 'po', 'la'],\n",
    "  ['za', 'bɛ', 'tu'],\n",
    "  ['ʁo', 'ki', 'sɛ'],\n",
    "  ['nu', 'ɡa', 'di']],\n",
    " [['dɛ', 'mʊ', 'ri'],\n",
    "  ['sɛ', 'ni', 'ɡɛ'],\n",
    "  ['ræ', 'ku', 'səʊ'],\n",
    "  ['pi', 'lɛ', 'ru']],\n",
    " [['ki', 'fəʊ', 'bu'],\n",
    "  ['lu', 'fɑ', 'ɡi'],\n",
    "  ['pæ', 'beɪ', 'lɑ'],\n",
    "  ['tɑ', 'ɡəʊ', 'fʊ']],\n",
    " [['bi', 'du', 'pɛ'],\n",
    "  ['məʊ', 'bɑ', 'li'],\n",
    "  ['rɛ', 'ɡæ', 'tʊ'],\n",
    "  ['sæ', 'tɛ', 'kəʊ']],\n",
    " [['bəʊ', 'dɑ', 'mɛ'],\n",
    "  ['fi', 'nəʊ', 'pɑ'],\n",
    "  ['ɡʊ', 'rɑ', 'təʊ'],\n",
    "  ['ləʊ', 'kæ', 'neɪ']],\n",
    " [['fɛ', 'si', 'nɑ'],\n",
    "  ['kɛ', 'su', 'dəʊ'],\n",
    "  ['mæ', 'pʊ', 'di'],\n",
    "  ['ti', 'mi', 'nu']],\n",
    " [['tu', 'pi', 'ɹoʊ'],\n",
    "  ['ɡoʊ', 'la', 'bu'],\n",
    "  ['pa', 'doʊ', 'ti'],\n",
    "  ['bi', 'da', 'ku']],\n",
    " [['meɪ', 'lu', 'ɡi'],\n",
    "  ['ɹa', 'fi', 'nu'],\n",
    "  ['pu', 'keɪ', 'mi'],\n",
    "  ['toʊ', 'na', 'poʊ']],\n",
    " [['ɡoʊ', 'la', 'tu'],\n",
    "  ['da', 'ɹoʊ', 'pi'],\n",
    "  ['ti', 'bu', 'doʊ'],\n",
    "  ['pa', 'bi', 'ku']],\n",
    " [['poʊ', 'fi', 'mu'],\n",
    "  ['noʊ', 'vu', 'ka'],\n",
    "  ['vi', 'koʊ', 'ɡa'],\n",
    "  ['ba', 'fu', 'ɡi']],\n",
    " [['ma', 'nu', 'toʊ'],\n",
    "  ['ni', 'moʊ', 'lu'],\n",
    "  ['voʊ', 'ɹi', 'fa'],\n",
    "  ['li', 'du', 'ɹa']]]\n",
    "\n",
    "ref_lexicons = list(map(to_lexicon, lexicons))\n",
    "\n",
    "for i, lex in enumerate(ref_lexicons):\n",
    "    print(i, lex)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Example Lexicon:\", ref_lexicons[0])\n",
    "print(\"Info:\", ref_lexicons[0].info)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_streams = Register()\n",
    "for _ in range(N_STREAMS_PER_INPUT):\n",
    "    for stream in make_streams(\n",
    "        ref_lexicons, \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        ):\n",
    "        ref_streams.append(stream)\n",
    "\n",
    "print_stream_info(ref_streams[0])\n",
    "\n",
    "len(ref_streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631123d",
   "metadata": {},
   "source": [
    "### Collect Results\n",
    "\n",
    "We collect all stream generation results and their feature repetitiveness scores in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e397-1ac5-4dac-84bb-106eaae80e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"Control\": [], \"Lexicon\": [], \"Feature\": [], \"PRI\": [], \"Stream TP mode\": [], \"Stream\": []}\n",
    "\n",
    "mode_to_mode = {  # TP-random position-random; TP-random position-fixed and TP-structured\n",
    "    \"random\": \"TP-random position-random\",\n",
    "    \"word_structured\": \"TP-structured\",\n",
    "    \"position_controlled\": \"TP-random position-fixed\"\n",
    "}\n",
    "\n",
    "for control, streams in [(\"Controlled lexicons (arpac)\", controlled_streams), (\"Reference lexicons (Literature)\", ref_streams), (\"Random lexicons (Baseline)\", random_streams)]:\n",
    "    for stream in streams:\n",
    "        for k, v in stream.info[\"rhythmicity_indexes\"].items():\n",
    "            data[\"Feature\"].append(k)\n",
    "            data[\"PRI\"].append(v)\n",
    "            data[\"Control\"].append(control)\n",
    "            data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "            data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "            data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "        data[\"Feature\"].append(\"max\")\n",
    "        data[\"PRI\"].append(max(stream.info[\"rhythmicity_indexes\"].values()))\n",
    "        data[\"Control\"].append(control)\n",
    "        data[\"Lexicon\"].append(str(stream.info[\"lexicon\"]))\n",
    "        data[\"Stream TP mode\"].append(mode_to_mode[stream.info[\"stream_tp_mode\"]])\n",
    "        data[\"Stream\"].append(\"|\".join(syll.id for syll in stream))\n",
    "\n",
    "df = pd.DataFrame(data).sort_values([\"Control\", \"Lexicon\", \"Stream TP mode\"]).reset_index(drop=True)\n",
    "\n",
    "import os\n",
    "os.makedirs(\"results/\", exist_ok=True)\n",
    "df.to_csv(\"results/full_dataset.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is always randomness in the generation of the lexicons etc., so if you want the exact data from the publication uncomment below \n",
    "# df = pd.read_csv(\"full_dataset.csv\")\n",
    "\n",
    "df_lexicons = df[[\"Control\", \"Lexicon\"]].drop_duplicates().reset_index(drop=True)\n",
    "df_lexicons.to_csv(\"results/all_lexicons.csv\")\n",
    "df_lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2653d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "from pingouin import ttest\n",
    "\n",
    "# There is always randomness in the generation of the lexicons etc., so if you want the exact data from the publication uncomment below \n",
    "# df = pd.read_csv(\"full_dataset.csv\")\n",
    "\n",
    "tp_modes_pretty = [\"TP-random position-random\", \"TP-random position-fixed\", \"TP-structured\"]\n",
    "dfs = []\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (arpac)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. reference {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Controlled lexicons (arpac)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"controlled vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for i, tp_mode in enumerate(tp_modes_pretty):\n",
    "    df2 = df[(df[\"Stream TP mode\"] == tp_mode) & (df[\"Feature\"] == \"max\")]\n",
    "    cat1 = df2[df2['Control']=='Reference lexicons (Literature)'][\"PRI\"]\n",
    "    cat2 = df2[df2['Control']=='Random lexicons (Baseline)'][\"PRI\"]\n",
    "    this = ttest(list(cat1), list(cat2), alternative=\"less\")\n",
    "    this.index = [f\"reference vs. random baseline {tp_mode}\"]\n",
    "    dfs.append(this)\n",
    "\n",
    "ttest_df = pd.concat(dfs)\n",
    "\n",
    "display(ttest_df)\n",
    "\n",
    "ttest_df.to_csv(\"results/ttest_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbff8f",
   "metadata": {},
   "source": [
    "## Example arpac Lexicon From Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_arpac_lexicon = to_lexicon([[\"heː\", \"doː\", \"faː\"], [\"riː\", \"foː\", \"ɡyː\"], [\"ʃuː\", \"hiː\", \"boː\"], [\"vaː\", \"kuː\", \"niː\"]])\n",
    "print(\"Example Lexicon:\", example_arpac_lexicon)\n",
    "\n",
    "streams = make_streams(\n",
    "        [example_arpac_lexicon], \n",
    "        max_rhythmicity=None, \n",
    "        num_repetitions=N_REPS\n",
    "        )\n",
    "\n",
    "for stream in streams:\n",
    "        print_stream_info(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d25f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arpac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
