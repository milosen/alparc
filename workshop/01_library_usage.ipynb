{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Controlled Stream Generation\n",
    "\n",
    "> ⚠️ We are in the process of migrating the old library called `arc` to a new library called `alparc`. While the command line tools carry the new name, the core library modules used in this and the following tutorials are still imported from `arc`. This will change in the future.\n",
    "\n",
    "We will generate words and a lexicon with minimal feature overlap between words. Next, we introduce the 3 main ways to generate random streams based on a lexicon. Each specifies how the transition probabilities (TPs) of their syllables are structured:\n",
    "\n",
    "1. uniformlly distributed TPs, called \"TP-random position-random\" in the paper, \n",
    "2. position-controlled TPs, called \"TP-random position-fixed\", and\n",
    "3. TPs that fully preserve the words, called \"TP-structured\".\n",
    "\n",
    "## Installation\n",
    "\n",
    "> ⚠️ We recommend using a virtual environment\n",
    "\n",
    "> ⚠️ If you use a virtual environment, make sure you use the right kernel for this notebook. You can usually select it in the top right corner. If your environment is not in the list, you have to add the ipython kernel from the environment like so:\n",
    "> 1. Activate virtual environment in terminal\n",
    "> 2. Run `pip install ipykernel`\n",
    "> 3. Run `python -m ipykernel install --user --name arc --display-name \"Python (ARC)\"`\n",
    "> 4. Reload this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5d22326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/nikola/workspace/alparc\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tyro in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (0.9.20)\n",
      "Requirement already satisfied: tqdm in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (4.66.1)\n",
      "Requirement already satisfied: seaborn in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (0.13.2)\n",
      "Requirement already satisfied: ruamel_yaml in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (0.17.32)\n",
      "Requirement already satisfied: pingouin in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (0.5.4)\n",
      "Requirement already satisfied: pydantic in /Users/nikola/miniforge3/lib/python3.10/site-packages (from alparc==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: numpy in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (1.13.1)\n",
      "Requirement already satisfied: pandas>=1.5 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (3.9.0)\n",
      "Requirement already satisfied: statsmodels in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (0.14.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (1.5.1)\n",
      "Requirement already satisfied: pandas-flavor in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (0.6.0)\n",
      "Requirement already satisfied: tabulate in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pingouin->alparc==0.0.1) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pydantic->alparc==0.0.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pydantic->alparc==0.0.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pydantic->alparc==0.0.1) (4.13.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from ruamel_yaml->alparc==0.0.1) (0.2.7)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from tyro->alparc==0.0.1) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from tyro->alparc==0.0.1) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from tyro->alparc==0.0.1) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from tyro->alparc==0.0.1) (4.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from matplotlib->pingouin->alparc==0.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pandas>=1.5->pingouin->alparc==0.0.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pandas>=1.5->pingouin->alparc==0.0.1) (2024.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from rich>=11.1.0->tyro->alparc==0.0.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from rich>=11.1.0->tyro->alparc==0.0.1) (2.18.0)\n",
      "Requirement already satisfied: xarray in /Users/nikola/miniforge3/lib/python3.10/site-packages (from pandas-flavor->pingouin->alparc==0.0.1) (2024.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from scikit-learn->pingouin->alparc==0.0.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from scikit-learn->pingouin->alparc==0.0.1) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from statsmodels->pingouin->alparc==0.0.1) (0.5.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nikola/miniforge3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->alparc==0.0.1) (0.1.2)\n",
      "Requirement already satisfied: six in /Users/nikola/miniforge3/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels->pingouin->alparc==0.0.1) (1.16.0)\n",
      "Building wheels for collected packages: alparc\n",
      "  Building editable for alparc (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alparc: filename=alparc-0.0.1-0.editable-py3-none-any.whl size=2592 sha256=5b829dec0f39fe144331ae82f272e423ff5aaec5c73fb0336fa3ff94e0ce0626\n",
      "  Stored in directory: /private/var/folders/n1/bxdrmv296493f6tbg9v8pjnh0000gn/T/pip-ephem-wheel-cache-utx_n5xx/wheels/4e/8b/0b/d3bdb9934e92c8d0097403841e577be8baadd88948ad73ee72\n",
      "Successfully built alparc\n",
      "Installing collected packages: alparc\n",
      "  Attempting uninstall: alparc\n",
      "    Found existing installation: alparc 0.0.1\n",
      "    Uninstalling alparc-0.0.1:\n",
      "      Successfully uninstalled alparc-0.0.1\n",
      "Successfully installed alparc-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --editable .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e0ed36c",
   "metadata": {},
   "source": [
    "\n",
    "## Syllable and Word Generation\n",
    "\n",
    "Because ARC runs probabilistically (to speed things up), we set the random seeds to make sure our runs are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1953ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɡ|k|b|d|p|t|x|ç|ʃ|f|... (38 elements total)\n",
      "ɡaː|ɡiː|ɡyː|ɡɛː|kaː|koː|kuː|køː|kɛː|baː|... (75 elements total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 825.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʃoːɡɛːmeː|reːfaːkoː|çaːroːbuː|heːʃoːpaː|myːseːɡɛː|ɡyːmɛːʃiː|reːɡaːfyː|niːpuːçaː|tiːçaːmeː|ʃøːɡaːmoː|... (10000 elements total)\n"
     ]
    }
   ],
   "source": [
    "from alparc import load_phonemes, make_syllables, make_words, make_lexicons, make_streams\n",
    "\n",
    "phonemes = load_phonemes()\n",
    "print(phonemes)\n",
    "\n",
    "syllables = make_syllables(phonemes)\n",
    "print(syllables)\n",
    "\n",
    "words = make_words(syllables)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c56bc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "words.save(os.path.join(\"results\", \"test_words\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0153360a",
   "metadata": {},
   "source": [
    "## Get Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1c605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_words in module arc.core.word:\n",
      "\n",
      "make_words(syllables: ~RegisterType, num_syllables=3, bigram_control=True, bigram_alpha=None, trigram_control=True, trigram_alpha=None, positional_control=True, positional_control_position=None, position_alpha=0, phonotactic_control=True, n_look_back=2, n_words=10000, max_tries=100000, progress_bar: bool = True) -> ~RegisterType\n",
      "    _summary_\n",
      "    \n",
      "    Args:\n",
      "        syllables (RegisterType): The Register of syllables to use as a basis for word generation\n",
      "        num_syllables (int, optional): how many syllables are in a word. Defaults to 3.\n",
      "        bigram_control (bool, optional): apply statistical control on the bigram level. Defaults to True.\n",
      "        bigram_alpha (_type_, optional): which p-value to assume for bigram control. Defaults to None.\n",
      "        trigram_control (bool, optional): apply statistical control on the trigram level. Defaults to True.\n",
      "        trigram_alpha (_type_, optional): which p-value to assume for trigram control. Defaults to None.\n",
      "        positional_control (bool, optional): control phoneme positions in words to be likely given the language. Defaults to True.\n",
      "        positional_control_position (int, optional): At which position to control phoneme likelihood (None means all). Defaults to None.\n",
      "        position_alpha (float, optional): probability throshold for positional control. Defaults to 0.\n",
      "        phonotactic_control (bool, optional): control each syllabel for minimum feature overlap with previous syllables. Defaults to True.\n",
      "        n_look_back (int, optional): how far to look back in the feature overlap control of the syllables. Defaults to 2.\n",
      "        n_words (_type_, optional): how many words to generate. Defaults to 10_000.\n",
      "        max_tries (_type_, optional): how often to attemt to add a word to the Register, before the function gives up. Defaults to 100_000.\n",
      "        progress_bar (bool, optional): print a progress bar based on 'n_words'. Defaults to True.\n",
      "    \n",
      "    Returns:\n",
      "        RegisterType: The Register of words.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(make_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fedf0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_syllables in module arc.core.syllable:\n",
      "\n",
      "make_syllables(phonemes: ~RegisterType, phoneme_pattern: str = 'cV', unigram_control: bool = True, language_control: bool = True, language_alpha: Optional[float] = 0.05, from_format: Literal['ipa', 'xsampa'] = 'xsampa', lang: str = 'deu') -> ~RegisterType\n",
      "    _summary_\n",
      "    \n",
      "    Args:\n",
      "        phonemes (RegisterType): A Register of phonemes that will be used as a basis to generate the syllables\n",
      "        phoneme_pattern (str, optional): describes how a syllable is structured, e.g. \"cV\" syllables consist of a single-consonant character and a long vowel. Defaults to \"cV\".\n",
      "        unigram_control (bool, optional): apply statistical control (on the basis of p-val of uniform distribution) to single unigrams. Defaults to True.\n",
      "        language_control (bool, optional): apply language specific controls (only german for now) on the syllable level. Defaults to True.\n",
      "        language_alpha (Optional[float], optional): which p-value to assume for language based statistical control. Defaults to 0.05.\n",
      "        from_format (Literal[&quot;ipa&quot;, &quot;xsampa&quot;], optional): language control will read from a syllable corpus. which format to assume. Defaults to \"xsampa\".\n",
      "        lang (str, optional): which language to use for language controls. Defaults to \"deu\".\n",
      "    \n",
      "    Returns:\n",
      "        RegisterType: The final Register of syllables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(make_syllables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31921911-61f2-4ffa-b254-be62abb90b37",
   "metadata": {},
   "source": [
    "## Lexicon Generation\n",
    "\n",
    "Now we generate lexica with minimal feature repetitiveness. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c873dd2-1854-4fa1-a320-a817523e7103",
   "metadata": {},
   "source": [
    "Let's generate 2 lexicons with 4 words each and print some info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6491055a-9398-44dc-8b61-b457bf928f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:alparc.core.lexicon:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=1\n",
      "WARNING:alparc.core.lexicon:Increasing allowed overlaps: MAX_PAIRWISE_OVERLAP=1, MAX_CUMULATIVE_OVERLAP=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 : høːboːsuː|doːfuːheː|buːçaːnyː|ɡyːløːfoː\n",
      "1 : muːtɛːçaː|faːkuːreː|seːpaːhuː|ɡiːluːfyː\n"
     ]
    }
   ],
   "source": [
    "from alparc import make_lexicons\n",
    "\n",
    "lexicons = make_lexicons(words, n_lexicons=2, n_words=4)\n",
    "print(\"\")\n",
    "\n",
    "for i, lexicon in enumerate(lexicons):\n",
    "    print(i, \":\", lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dae4bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_lexicons in module alparc.core.lexicon:\n",
      "\n",
      "make_lexicons(words: ~RegisterType, n_lexicons: int = 5, n_words: int = 4, max_overlap: int = 1, lag_of_interest: int = 1, max_word_matrix: int = 200, unique_words: bool = False, binary_feature_control: bool = True, progress_bar: bool = False, control_features: List[Literal['syl', 'son', 'cons', 'cont', 'delrel', 'lat', 'nas', 'strid', 'voi', 'sg', 'cg', 'ant', 'cor', 'distr', 'lab', 'hi', 'lo', 'back', 'round', 'tense', 'long']] = ['syl', 'son', 'cons', 'cont', 'delrel', 'lat', 'nas', 'strid', 'voi', 'sg', 'cg', 'ant', 'cor', 'distr', 'lab', 'hi', 'lo', 'back', 'round', 'tense', 'long']) -> List[alparc.types.base_types.Register]\n",
      "    _summary_\n",
      "    \n",
      "    Args:\n",
      "        words (RegisterType): The Register of words which the lexicon generation is based on.\n",
      "        n_lexicons (int, optional): How many lexicons to generate. Defaults to 5.\n",
      "        n_words (int, optional): How many words should be in a lexicon. Defaults to 4.\n",
      "        max_overlap (int, optional): How much feature overlap between pairwise word features is allowed. Defaults to 1.\n",
      "        lag_of_interest (int, optional): the frequency of the word features for which a feature is consideret 'overlapping'. 1 means the feature frequency is the number of syllables in 1 word. Defaults to 1.\n",
      "        max_word_matrix (int, optional): How many words to use maximum (subsample if nessesary) to generate the feature overlap matrix. Defaults to 200.\n",
      "        unique_words (bool, optional): check uniqueness of words across all lexicons. Defaults to False.\n",
      "        binary_feature_control (bool): control feature overlap between words in the lexicon. If false lexicons will be generated completely at random. Defaults to True.\n",
      "    \n",
      "    Returns:\n",
      "        List[Lexicon]: A List of Lexicons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(make_lexicons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a7816d0-f88c-4353-ad5a-e7a137eddd51",
   "metadata": {},
   "source": [
    "> ⚠️ The runtime of this function depends on the parameters when `control_features=True`. If it takes too long, consider reducing the number of words in the lexicon or the number of lexicons. If you don't get any output, consider increasing the maximum pairwise overlap allowed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0c5caf",
   "metadata": {},
   "source": [
    "By default, Lexicons with the minimum possible cumulative feature repetitiveness will be generated first, starting at zero. This means words will be joined into a lexicon if the features of all word pairs in the lexicon have no overlap. If it is not possible to generate the requested number Lexicons with zero overlap, the allowed overlap will be increased untill all lexicons are collected, which will be indicated by a warning message.\n",
    "\n",
    "This process will be repeated, until any of the following statements is true\n",
    "- the requested number of Lexicons has been generated\n",
    "- the maximum allowed overlap is reached (set via `max_overlap`)\n",
    "- the set of all word combinations is exhausted\n",
    "\n",
    "If one or more Lexicons is returned, their info fields hold the cumulative overlap between all word pairs that is achieved by the Lexicon as well as the maximum pairwise overlap used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab5ca7e-5929-446b-8a9e-78c979d3b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: høːboːsuː|doːfuːheː|buːçaːnyː|ɡyːløːfoː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n",
      "Lexicon: muːtɛːçaː|faːkuːreː|seːpaːhuː|ɡiːluːfyː\n",
      "cumulative_feature_repetitiveness: 2\n",
      "max_pairwise_feature_repetitiveness: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lexicon in lexicons:\n",
    "    print(\"Lexicon:\", lexicon)\n",
    "    print(\"cumulative_feature_repetitiveness:\", lexicon.info[\"cumulative_feature_repetitiveness\"])\n",
    "    print(\"max_pairwise_feature_repetitiveness:\", lexicon.info[\"max_pairwise_feature_repetitiveness\"])\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9186f7db",
   "metadata": {},
   "source": [
    "## Stream Generation\n",
    "\n",
    "We want to generate a complete set of compatible lexicons for our study, i.e. to generate a compatible set of streams for testing statistical learning hypotheses. If `streams` is empty, try increasing the allowed maximum rythmicity.\n",
    "\n",
    "The function `make_streams` will try to generate one stream for each lexicon and TP mode. If you specify 'max_rhythmicity', it will discard those that do not meet the requirement. By default, all streams from a lexicon will be discarded if the lexicon can't generate streams for all requested TP modes. Printed below you see a collection of streams. Because streams can get long, you only see their key consisting of the lexicon used to generate it and its TP mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f69ef37-8b48-4340-b9f8-63a0777dde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_streams in module alparc.core.stream:\n",
      "\n",
      "make_streams(lexicons: Optional[List[~RegisterType]], max_rhythmicity: Optional[float] = None, stream_length: int = 15, max_tries_randomize: int = 10, tp_modes: tuple = ('random', 'word_structured', 'position_controlled'), require_all_tp_modes: bool = True) -> ~RegisterType\n",
      "    _summary_\n",
      "    \n",
      "    Args:\n",
      "        lexicons (List[LexiconType]): A list of lexicons used as a basis for generatng the streams\n",
      "        max_rhythmicity (Optional[float], optional): check rhythmicity and discard all streams that have at least one feature with higher PRI than this number. Defaults to None.\n",
      "        stream_length (int, optional): how many syllables are in a stream in multiples of syllables in the lexicon. Defaults to 4.\n",
      "        max_tries_randomize (int, optional): if max_rhythmicity is given and violated, how many times to try with a new randomization. Defaults to 10.\n",
      "        tp_modes (tuple, optional): the ways (modes) in which to control for transition probabilities of syllables in the stream. Defaults to (\"random\", \"word_structured\", \"position_controlled\").\n",
      "        require_all_tp_modes (bool, optional): all streams coming from the same lexicon will be discarded if not all their tp-modes have been found. Defaults to True.\n",
      "    \n",
      "    Returns:\n",
      "        RegisterType: _description_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from alparc import make_streams\n",
    "help(make_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b0c7f0-281a-4f02-8ac9-3b1621b8a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = make_streams(lexicons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac301ad8-f883-4de7-9719-7db125b37570",
   "metadata": {},
   "source": [
    "> ⚠️ The runtime of this function depends on the parameters, especially when you specify a `max_rhythmicity`, because the function re-samples the random stream until `max_rhythmicity` is satisfied. This takes time, because TP-statistics need to be controlled each time. If it takes too long, consider removing the option.\n",
    "\n",
    "To inspect a stream, select one either by index or by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d224f751-3cc9-4549-b6e7-b3697f11b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "høː_çaː_foː_buː_suː...høː_boː_løː_nyː_buː\n"
     ]
    }
   ],
   "source": [
    "stream = streams[0]\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65e606a1-8427-4d09-9058-f2c0e330b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: høːboːsuː|doːfuːheː|buːçaːnyː|ɡyːløːfoː\n",
      "TP mode: random\n",
      "Feature PRIs:\n",
      "  phon_1_son 0.047619047619047616\n",
      "  phon_1_back 0.0\n",
      "  phon_1_hi 0.03221288515406162\n",
      "  phon_1_lab 0.0938375350140056\n",
      "  phon_1_cor 0.0700280112044818\n",
      "  phon_1_cont 0.029411764705882353\n",
      "  phon_1_lat 0.0\n",
      "  phon_1_nas 0.0\n",
      "  phon_1_voi 0.04341736694677871\n",
      "  phon_2_back 0.028011204481792718\n",
      "  phon_2_hi 0.08263305322128851\n",
      "  phon_2_lo 0.0\n",
      "  phon_2_lab 0.08263305322128851\n",
      "  phon_2_tense 0.0\n",
      "  phon_2_long 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Lexicon:\", stream.info[\"lexicon\"])\n",
    "print(\"TP mode:\", stream.info[\"stream_tp_mode\"])\n",
    "print(\"Feature PRIs:\") \n",
    "for feat, pri in stream.info[\"rhythmicity_indexes\"].items():\n",
    "    print(\" \", feat, pri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7558308-40fa-4fb5-8ec1-585fc73ba250",
   "metadata": {},
   "source": [
    "As you can see, the `.info` field holds some useful information about the generated stream, i.e. which Lexicon has been used to generate it, the rythmicity indexes achieved for each feature, and which randomization/TP-structure mode has been used.\n",
    "\n",
    "This concludes the second tutorial, and we end this series with the third and last tutorial about how to use your own data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
