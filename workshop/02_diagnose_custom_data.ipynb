{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0859a1-27fb-4496-b4fd-f87ab45ea567",
   "metadata": {},
   "source": [
    "# Use your own data\n",
    "\n",
    "In this tutorial, we discuss two ways of using your own data:\n",
    "\n",
    "1. You have one or more lexicons you want to evaluate and generate streams with\n",
    "2. You already have streams and just want to evaluate them\n",
    "\n",
    "If you want to expand ARC, we are happy to invite you to contribute to the [ARC Project](https://github.com/milosen/arc) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa8e43d8-f3f3-4d6b-a756-bb8bb96fc67a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading/creating your custom lexicon\n",
    "\n",
    "Let's say you have a lexicon consisting of the (pseudo-)words 'piɾuta', 'baɡoli', 'tokuda, and 'ɡuhaɪbo'.\n",
    "\n",
    "We assume you have prepared your lexicon as a list of lists (see below), and that all syllables are of the same type. The function `to_lexicon()` accepts the syllable types we call 'cv' and 'cV'. 'cv' is a syllable consisting of a single-character consonant and a short vowel, e.g. 'pi'. Because it is common in the literature, 'cv' also allows diphthongs, e.g. 'haɪ'). The 'cV' type is a single-character consonant, together with a long vowel, e.g. 'tuː'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b20f17a-ff7b-46cf-bc4c-7f341ba8adc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon: piɾuta|baɡoli|tokuda|ɡuhaɪbo\n",
      "\n",
      "syllables_info: {'syllable_feature_labels': [['son', 'back', 'hi', 'lab', 'cor', 'cont', 'lat', 'nas', 'voi'], ['back', 'hi', 'lo', 'lab', 'tense', 'long']], 'syllable_type': 'cv'}\n",
      "cumulative_feature_repetitiveness: 7\n",
      "max_pairwise_feature_repetitiveness: 2\n"
     ]
    }
   ],
   "source": [
    "from alparc import to_lexicon\n",
    "import numpy as np\n",
    "\n",
    "raw_lexicon = [\n",
    "  ['pi', 'ɾu', 'ta'],\n",
    "  ['ba', 'ɡo', 'li'],\n",
    "  ['to', 'ku', 'da'],\n",
    "  ['ɡu', 'haɪ', 'bo']\n",
    "]\n",
    "\n",
    "lexicon = to_lexicon(raw_lexicon, syllable_type=\"cv\")\n",
    "\n",
    "print(\"Lexicon:\", lexicon)\n",
    "print(\"\")\n",
    "\n",
    "for key, value in lexicon.info.items():\n",
    "    print(f\"{key}:\", lexicon.info[key])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19210d47-441f-43fc-ac87-c7eee58252b7",
   "metadata": {},
   "source": [
    "### 1.1. Custom Lexicon: Moving upstream\n",
    "\n",
    "Now we \"move upstream\" in the generation process. We turn the lexicon into a stream using the standard `alparc` functions introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e8de424-9c86-40a3-b709-32ad6278ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streams (summary): piɾutabaɡolitokudaɡuhaɪbo_random|piɾutabaɡolitokudaɡuhaɪbo_word_structured|piɾutabaɡolitokudaɡuhaɪbo_position_controlled\n",
      "\n",
      "tp_modes: ('random', 'word_structured', 'position_controlled')\n",
      "max_rhythmicity: None\n",
      "max_tries_randomize: 10\n",
      "stream_length: 15\n",
      "require_all_tp_modes: True\n"
     ]
    }
   ],
   "source": [
    "from alparc import make_streams\n",
    "streams = make_streams([lexicon])\n",
    "\n",
    "print(\"Streams (summary):\", streams)\n",
    "print(\"\")\n",
    "\n",
    "for key, value in streams.info.items():\n",
    "    print(f\"{key}:\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d839edd9-9d33-423b-a22c-e99be09edc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream (random):  ta_to_ba_pi_ɡo...bo_haɪ_da_ku_to\n",
      "PRIs:\n",
      "  phon_1_son 0.058823529411764705\n",
      "  phon_1_back 0.08263305322128851\n",
      "  phon_1_hi 0.08263305322128851\n",
      "  phon_1_lab 0.07282913165266107\n",
      "  phon_1_cor 0.06862745098039216\n",
      "  phon_1_cont 0.058823529411764705\n",
      "  phon_1_lat 0.004201680672268907\n",
      "  phon_1_nas 0.0\n",
      "  phon_1_voi 0.02100840336134454\n",
      "  phon_2_back 0.0\n",
      "  phon_2_hi 0.06162464985994398\n",
      "  phon_2_lo 0.12745098039215685\n",
      "  phon_2_lab 0.05742296918767507\n",
      "  phon_2_tense 0.0\n",
      "  phon_2_long 0.0\n",
      "Max PRI across features: phon_2_lo 0.12745098039215685\n",
      "Cummulative PRI across features: 0.696078431372549\n",
      " \n",
      "Stream (word_structured):  pi_ɾu_ta_to_ku...ɡo_li_to_ku_da\n",
      "PRIs:\n",
      "  phon_1_son 0.1400560224089636\n",
      "  phon_1_back 0.14705882352941177\n",
      "  phon_1_hi 0.14705882352941177\n",
      "  phon_1_lab 0.12044817927170869\n",
      "  phon_1_cor 0.04481792717086835\n",
      "  phon_1_cont 0.1400560224089636\n",
      "  phon_1_lat 0.0\n",
      "  phon_1_nas 0.0\n",
      "  phon_1_voi 0.011204481792717087\n",
      "  phon_2_back 0.0\n",
      "  phon_2_hi 0.011204481792717087\n",
      "  phon_2_lo 0.09663865546218488\n",
      "  phon_2_lab 0.1400560224089636\n",
      "  phon_2_tense 0.0\n",
      "  phon_2_long 0.0\n",
      "Max PRI across features: phon_1_back 0.14705882352941177\n",
      "Cummulative PRI across features: 0.9985994397759104\n",
      " \n",
      "Stream (position_controlled):  ba_ɾu_li_to_haɪ...ɡo_li_ba_ɾu_bo\n",
      "PRIs:\n",
      "  phon_1_son 0.11484593837535013\n",
      "  phon_1_back 0.09943977591036414\n",
      "  phon_1_hi 0.09943977591036414\n",
      "  phon_1_lab 0.09663865546218488\n",
      "  phon_1_cor 0.12184873949579832\n",
      "  phon_1_cont 0.11484593837535013\n",
      "  phon_1_lat 0.02100840336134454\n",
      "  phon_1_nas 0.0\n",
      "  phon_1_voi 0.025210084033613446\n",
      "  phon_2_back 0.0\n",
      "  phon_2_hi 0.036414565826330535\n",
      "  phon_2_lo 0.06302521008403361\n",
      "  phon_2_lab 0.08403361344537816\n",
      "  phon_2_tense 0.0\n",
      "  phon_2_long 0.0\n",
      "Max PRI across features: phon_1_cor 0.12184873949579832\n",
      "Cummulative PRI across features: 0.8767507002801121\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for stream in streams:\n",
    "    tp_mode = stream.info['stream_tp_mode']\n",
    "    pris = stream.info['rhythmicity_indexes']\n",
    "    \n",
    "    print(f\"Stream ({tp_mode}): \", stream)\n",
    "    print(\"PRIs:\")\n",
    "    max = \"phon_1_son\"\n",
    "    cum = 0\n",
    "    for feat, pri in stream.info[\"rhythmicity_indexes\"].items():\n",
    "        print(\" \", feat, pri)\n",
    "        if pri > stream.info[\"rhythmicity_indexes\"][max]:\n",
    "            max = feat\n",
    "        cum += pri\n",
    "\n",
    "    print(\"Max PRI across features:\", max, stream.info[\"rhythmicity_indexes\"][max])\n",
    "    print(\"Cummulative PRI across features:\", cum)\n",
    "    print(\" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "703f58eb-7992-4622-a894-489101abbd8b",
   "metadata": {},
   "source": [
    "### 1.2. Custom Lexicon: Moving backwards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e248df26-3b8e-45a7-858a-6cebcf62b602",
   "metadata": {},
   "source": [
    "\"moving backwards\" in the generation process, i.e. generating words, syllables, and phonemes is less common, but we got you covered. Let's say you want to compare the syllables in your custom lexicon with the arc corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdde46cd-93cd-4f21-9d46-d8a5ff6f93ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi|ɾu|ta|ba|ɡo|li|to|ku|da|ɡu|... (12 elements total)\n"
     ]
    }
   ],
   "source": [
    "syllables = lexicon.flatten()\n",
    "print(syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5f30f0-4488-432a-824a-524c8dc65d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi|ta|ba|ɡo|li|to|ku|da|ɡu|haɪ|... (11 elements total)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'binary_features': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " 'phonotactic_features': [['plo', 'lab'], ['i']],\n",
       " 'freq': 70,\n",
       " 'prob': 6.92628e-05}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alparc.io import read_syllables_corpus\n",
    "\n",
    "corpus_syllables = read_syllables_corpus()\n",
    "\n",
    "syllables_with_corpus_stats = syllables.intersection(corpus_syllables)\n",
    "\n",
    "print(syllables_with_corpus_stats)\n",
    "syllables_with_corpus_stats[\"pi\"].info\n",
    "\n",
    "#note: mention that frew and prob are new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69e19df-e1b8-4d62-93b9-5cd9921849cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p|i|ɾ|u|t|a|b|ɡ|o|l|... (13 elements total)\n"
     ]
    }
   ],
   "source": [
    "phonemes = syllables.flatten()\n",
    "print(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d74536ce-2c78-40e2-84cf-faf7dd77aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only german phoneme corpus available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p|i|ɾ|u|t|a|b|ɡ|o|l|... (13 elements total)\n",
      "{'features': ['-', '-', '+', '-', '-', '-', '-', '0', '-', '-', '-', '+', '-', '0', '+', '-', '-', '-', '-', '0', '-']}\n"
     ]
    }
   ],
   "source": [
    "from alparc.io import read_phoneme_corpus\n",
    "corpus_phonemes = read_phoneme_corpus()\n",
    "\n",
    "phonemes_with_stats = phonemes.intersection(corpus_phonemes)\n",
    "print(phonemes_with_stats)\n",
    "print(phonemes_with_stats[\"p\"].info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b6e2702-7363-45b3-a289-2dc051d29570",
   "metadata": {},
   "source": [
    "## Evaluating your stream\n",
    "\n",
    "Again, we assume you have prepared your data into a list of syllables like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5e7334-cfc1-4fd4-bc76-08edfefabb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream:  pi_ɾu_ta_ba_ɡo...ku_da_ɡu_ki_bo\n",
      "\n",
      "rhythmicity indexes (PRIs) {'phon_1_son': 0.0, 'phon_1_back': 0.1724137931034483, 'phon_1_hi': 0.1724137931034483, 'phon_1_lab': 0.08620689655172414, 'phon_1_cor': 0.0, 'phon_1_cont': 0.0, 'phon_1_lat': 0.0, 'phon_1_nas': 0.0, 'phon_1_voi': 0.0, 'phon_2_back': 0.0, 'phon_2_hi': 0.0, 'phon_2_lo': 0.0, 'phon_2_lab': 0.0, 'phon_2_tense': 0.0, 'phon_2_long': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from alparc import to_stream\n",
    "\n",
    "stream = ['pi', 'ɾu', 'ta', 'ba', 'ɡo', 'li', 'to', 'ku', 'da', 'ɡu', 'ki', 'bo']*streams.info['stream_length']\n",
    "\n",
    "stream = to_stream(stream)\n",
    "\n",
    "print(\"Stream: \", stream, end=\"\\n\\n\")\n",
    "print(\"rhythmicity indexes (PRIs)\", stream.info['rhythmicity_indexes'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e460fa40-acf5-48a9-b00a-abd0b8f5f970",
   "metadata": {},
   "source": [
    "As you can see, even with a custom lexicon, the randomization of a stream has an effect on the PRIs.\n",
    "\n",
    "This concludes our third and last tutorial. We hope you feel ready to use ARC, and help us extend it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
